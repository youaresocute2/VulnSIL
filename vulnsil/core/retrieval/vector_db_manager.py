# vulnsil/core/retrieval/vector_db_manager.py
import numpy as np
import torch
import logging
import threading
import queue
import os

try:
    import pynvml

    pynvml.nvmlInit()
    HAS_GPU = True
except:
    HAS_GPU = False

from transformers import AutoTokenizer, AutoModel
from config import settings

log = logging.getLogger(__name__)


class EmbeddingModel:
    """
    Hybrid Resource Pool for Embedding (GPU + CPU).
    """

    def __init__(self):
        # [å…³é”®] é™åˆ¶ PyTorch ç®—å­çº§å¹¶è¡Œï¼Œé˜²æ­¢å¹²æ‰° Pipeline ä¸»çº¿ç¨‹
        torch.set_num_threads(1)

        self.model_path = settings.EMBEDDING_MODEL_PATH
        self.worker_pool = queue.Queue()

        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        except Exception as e:
            log.critical(f"Tokenizer load fail: {e}")
            raise

        loaded_count = 0

        # è§£æžé…ç½®ä¸­çš„è®¾å¤‡åˆ—è¡¨
        target_devices = []
        if settings.EMBEDDING_DEVICE:
            target_devices = [d.strip() for d in settings.EMBEDDING_DEVICE.split(',')]

        # é»˜è®¤å…œåº•é€»è¾‘
        if not target_devices:
            target_devices = ['cuda:0', 'cpu'] if HAS_GPU else ['cpu']

        log.info(f"ðŸ”§ Embedding Device Config: {target_devices}")

        for d_str in target_devices:
            if d_str == 'cpu':
                # [ä¿®æ”¹] CPU Worker æ•°é‡è®¾ä¸º 4ã€‚
                # é…åˆ Pipeline çš„ 16 çº¿ç¨‹ï¼Œé¿å… CPU è¿‡è½½ã€‚
                # 1ä¸ª GPU Worker (å¤„ç†çº¦ 60% æµé‡) + 4ä¸ª CPU Worker (å¤„ç†çº¦ 40% æµé‡)
                cpu_workers = 6
                log.info(f"ðŸš€ Spawning {cpu_workers} CPU workers for Embedding...")
                for _ in range(cpu_workers):
                    if self._load_worker('cpu'):
                        loaded_count += 1
            else:
                # GPU è®¾å¤‡ (cuda:0)
                if d_str.startswith("cuda") and not HAS_GPU:
                    log.warning(f"âš ï¸ Configured {d_str} but no GPU detected. Skipping.")
                    continue
                # GPU Worker é€šå¸¸ 1 ä¸ªå°±å¤Ÿäº†ï¼Œåžåé‡æžå¤§
                if self._load_worker(d_str):
                    loaded_count += 1

        if loaded_count == 0:
            log.warning("âš ï¸ No workers loaded! Falling back to single CPU worker.")
            self._load_worker("cpu")
            loaded_count = 1

        self.active_ids = [i for i in range(loaded_count)]
        log.info(f"ðŸš€ Embedding Pool Ready. Total Workers: {loaded_count}")

    def _load_worker(self, device_str):
        try:
            dev = torch.device(device_str)
            model = AutoModel.from_pretrained(self.model_path).to(dev)
            model.eval()

            if dev.type == 'cuda':
                model.half()  # GPU FP16
            else:
                # CPU FP32 (éƒ¨åˆ† CPU ä¸æ”¯æŒåŠç²¾åº¦)
                pass

            self.worker_pool.put((model, dev))
            return True
        except Exception as e:
            log.error(f"âŒ Failed to load model on {device_str}: {e}")
            return False

    def get_active_devices(self):
        return self.active_ids

    @torch.no_grad()
    def encode(self, text: str) -> np.ndarray:
        if not text or not text.strip():
            return np.zeros(768, dtype='float32')

        text = text[:5000]

        # ä»Žèµ„æºæ± èŽ·å–æ¨¡åž‹ (é˜»å¡žç­‰å¾…)
        model, device = self.worker_pool.get()

        try:
            inputs = self.tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)
            inputs = {k: v.to(device) for k, v in inputs.items()}

            outputs = model(**inputs)
            embedding = outputs.last_hidden_state[:, 0, :]
            embedding = embedding.float().cpu().numpy()[0].astype('float32')

            norm = np.linalg.norm(embedding)
            if norm > 1e-10: embedding /= norm
            return embedding

        except Exception as e:
            log.error(f"Encoding Error on {device}: {e}")
            return np.zeros(768, dtype='float32')

        finally:
            # å½’è¿˜æ¨¡åž‹
            self.worker_pool.put((model, device))