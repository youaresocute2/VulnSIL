# vulnsil/core/retrieval/vector_db_manager.py
import numpy as np
import torch
import logging
import queue
import os

# å°è¯•æ£€æµ‹ GPU çŽ¯å¢ƒ
try:
    import pynvml

    pynvml.nvmlInit()
    HAS_GPU = True
except:
    HAS_GPU = False

from transformers import AutoTokenizer, AutoModel
from config import settings

log = logging.getLogger(__name__)


class EmbeddingModel:
    """
    Embedding Model Resource Pool.
    Supports Hybrid (GPU + CPU) deployment strategies for high-throughput encoding.
    """

    def __init__(self):
        # [Optimization] é™åˆ¶ PyTorch å†…éƒ¨ç®—å­å¹¶è¡Œçº¿ç¨‹æ•°
        # é˜²æ­¢ä¸Ž Pipeline çš„ ThreadPoolExecutor (8-16 çº¿ç¨‹) å†²çªå¯¼è‡´ Context Switching è¿‡é«˜
        torch.set_num_threads(1)

        self.model_path = settings.EMBEDDING_MODEL_PATH
        self.worker_pool = queue.Queue()

        try:
            # Tokenizer æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œå…¨å±€åŠ è½½ä¸€æ¬¡
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        except Exception as e:
            log.critical(f"Tokenizer load failed at {self.model_path}: {e}")
            raise

        loaded_count = 0

        # è§£æžé…ç½®ä¸­çš„è®¾å¤‡åˆ—è¡¨ "cuda:0,cpu"
        target_devices = []
        if settings.EMBEDDING_DEVICE:
            target_devices = [d.strip() for d in settings.EMBEDDING_DEVICE.split(',')]

        # å…œåº•é»˜è®¤é€»è¾‘
        if not target_devices:
            target_devices = ['cuda:0'] if HAS_GPU else ['cpu']

        log.info(f"ðŸ”§ Embedding Setup | Target Devices: {target_devices}")

        # åŠ è½½ Worker
        for d_str in target_devices:
            if d_str == 'cpu':
                # CPU åžåè¾ƒå¼±ï¼Œä¸ºäº†é˜²æ­¢ bottleneckï¼Œé€šå¸¸å¯åŠ¨å¤šä¸ª CPU Workers
                cpu_worker_count = 4
                log.info(f"ðŸš€ Spawning {cpu_worker_count} CPU workers...")
                for _ in range(cpu_worker_count):
                    if self._load_worker('cpu'):
                        loaded_count += 1
            else:
                # GPU Worker
                if d_str.startswith("cuda") and not HAS_GPU:
                    log.warning(f"âš ï¸ Device {d_str} requested but NVML says No GPU. Skipping.")
                    continue
                if self._load_worker(d_str):
                    loaded_count += 1

        if loaded_count == 0:
            log.warning("âš ï¸ No devices loaded! Falling back to single CPU worker safety mode.")
            self._load_worker("cpu")
            loaded_count = 1

        log.info(f"âœ… Embedding Pool Ready. Total Workers: {loaded_count}")

    def _load_worker(self, device_str):
        """Helper to load model onto specific device"""
        try:
            dev = torch.device(device_str)
            model = AutoModel.from_pretrained(self.model_path).to(dev)
            model.eval()

            # GPU åŠç²¾åº¦ä¼˜åŒ– (FP16)
            if dev.type == 'cuda':
                model.half()

                # å°†æ¨¡åž‹æ”¾å…¥æ± ä¸­ (Model, Device)
            self.worker_pool.put((model, dev))
            return True
        except Exception as e:
            log.error(f"âŒ Load failed on {device_str}: {e}")
            return False

    @torch.no_grad()
    def encode(self, text: str) -> np.ndarray:
        """
        Encode text to 768-dim vector.
        Thread-safe wrapper around resource pool.
        """
        if not text or not text.strip():
            # Return zero vector for empty inputs to maintain pipeline stability
            return np.zeros(768, dtype='float32')

        # Limit extreme lengths for Bert
        text = text[:8000]

        # Borrow worker (Block until available)
        model, device = self.worker_pool.get()

        try:
            inputs = self.tokenizer(
                text,
                return_tensors='pt',
                max_length=512,
                truncation=True,
                padding=True
            )
            inputs = {k: v.to(device) for k, v in inputs.items()}

            outputs = model(**inputs)
            # CLS pooling
            embedding = outputs.last_hidden_state[:, 0, :]

            # Move to CPU / NumPy
            embedding_np = embedding.float().cpu().numpy()[0].astype('float32')

            # Normalize (Cosine Similarity Prep)
            norm = np.linalg.norm(embedding_np)
            if norm > 1e-10:
                embedding_np /= norm

            return embedding_np

        except Exception as e:
            log.error(f"Encoding Error on {device}: {e}")
            return np.zeros(768, dtype='float32')

        finally:
            # Return worker
            self.worker_pool.put((model, device))