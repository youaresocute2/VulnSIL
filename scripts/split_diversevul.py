# scripts/split_diversevul.py
import json
import os
import hashlib
import pandas as pd
import numpy as np
import sys
from collections import Counter

# === è·¯å¾„é…ç½® ===
# è¯·ç¡®è®¤è¿™æ˜¯æ‚¨æœ¬åœ°çš„åŸå§‹æ•°æ®é›†è·¯å¾„
INPUT_FILE = "data/diversevul.json"
# è¾“å‡ºç›®å½•
OUTPUT_DIR = "data/dataset_final"

# éšæœºç§å­ï¼Œä¿è¯æ¯æ¬¡è¿è¡Œç»“æœä¸€è‡´
SEED = 42

os.makedirs(OUTPUT_DIR, exist_ok=True)


def get_hash_split_label(commit_id, ratio=0.8):
    """
    Commit-level ç‰©ç†éš”ç¦» (Hashå–æ¨¡)
    """
    if not commit_id: return "train"
    # ä½¿ç”¨ sha256 é¿å… MD5 ç¢°æ’é£é™©ï¼Œè™½ç„¶åœ¨å–æ¨¡100çš„æƒ…å†µä¸‹åŒºåˆ«ä¸å¤§
    h = int(hashlib.sha256(str(commit_id).encode('utf-8')).hexdigest(), 16)
    return "train" if (h % 100) < (ratio * 100) else "test"


def clean_cwe_for_stats(raw_val):
    """
    ä»…ç”¨äºPandasç»Ÿè®¡å’Œåˆ†å±‚æŠ½æ ·è®¡ç®—ï¼Œä¸ä¿®æ”¹åŸå§‹æ•°æ®ç»“æ„
    """
    if isinstance(raw_val, list) and len(raw_val) > 0:
        val = str(raw_val[0])
    elif isinstance(raw_val, str):
        val = raw_val
    else:
        val = "Other"

    val = val.strip().upper()
    if val in ["NON", "N/A", "NONE", "NULL", "", "[]"]: return "Other"
    return val


def print_detailed_report(df, name):
    """
    è¯¦ç»†çš„æ•°æ®ç»Ÿè®¡é¢æ¿
    """
    total = len(df)
    if total == 0:
        print(f"\nâŒ [{name}] is Empty!")
        return

    n_vuln = len(df[df['target'] == 1])
    n_safe = len(df[df['target'] == 0])

    vuln_rate = (n_vuln / total) * 100
    safe_rate = (n_safe / total) * 100

    ratio_str = f"1 : {n_safe / n_vuln:.2f}" if n_vuln > 0 else "N/A"

    print("\n" + "=" * 60)
    print(f" ğŸ“Š DATASET REPORT: {name}")
    print("=" * 60)
    print(f" ğŸ“¦ Total Rows : {total}")
    print(f" ğŸ”´ Vuln (1)   : {n_vuln:<8} ({vuln_rate:.2f}%)")
    print(f" ğŸŸ¢ Safe (0)   : {n_safe:<8} ({safe_rate:.2f}%)")
    print(f" âš–ï¸  Real Ratio : {ratio_str} (Vuln:Safe)")

    # CWE åˆ†å¸ƒå±•ç¤º (åªåœ¨æœ‰æ¼æ´æ—¶å±•ç¤º)
    if n_vuln > 0:
        # ä¸ºäº†ç»Ÿè®¡ï¼Œä¸´æ—¶æ¸…æ´—ä¸€åˆ—
        cwe_series = df[df['target'] == 1]['cwe'].apply(clean_cwe_for_stats)
        counts = cwe_series.value_counts()
        print("-" * 60)
        print(f" ğŸ” Top 10 CWE Breakdown (Positive Samples Only)")
        print("-" * 60)
        for cwe, count in counts.head(10).items():
            pct = (count / n_vuln) * 100
            print(f"   - {cwe:<20} : {count:<6} ({pct:.2f}%)")
    print("=" * 60)


def main():
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ Error: Input file not found: {INPUT_FILE}")
        sys.exit(1)

    print(f"ğŸš€ Loading raw data from {INPUT_FILE}...")

    train_buffer = []
    test_buffer = []

    # 1. è¯»å– + Commit çº§å“ˆå¸Œéš”ç¦»
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line: continue
            try:
                obj = json.loads(line)

                # å…³é”®å­—æ®µå®Œæ•´æ€§æ ¡éªŒ
                if 'target' not in obj:
                    obj['target'] = int(obj.get('label', 0))  # å…¼å®¹ label å­—æ®µ
                else:
                    obj['target'] = int(obj['target'])

                commit_id = obj.get('commit_id', "")

                # Hash åˆ¤æ–­ Split
                label = get_hash_split_label(commit_id)

                if label == "train":
                    train_buffer.append(obj)
                else:
                    test_buffer.append(obj)
            except Exception as e:
                continue

    # è½¬ DataFrame æ–¹ä¾¿æ“ä½œï¼Œä½†å­˜å‚¨æ—¶è¿˜æ˜¯å†™ JSONL
    df_train = pd.DataFrame(train_buffer)
    df_test = pd.DataFrame(test_buffer)

    print("\nâœ… Strict Isolation Split Completed.")

    # 2. ä¿å­˜ Full Splits (ç¬¦åˆ manage_database è¦æ±‚çš„ jsonl)
    # [Train Full] -> ç”¨ä½œ RAG çŸ¥è¯†åº“æº (RAG Builder ä¼šè‡ªåŠ¨æå–å…¶ä¸­çš„ 1)
    file_train_full = os.path.join(OUTPUT_DIR, "train_full.jsonl")
    df_train.to_json(file_train_full, orient='records', lines=True, force_ascii=False)

    # [Test Full] -> ç”¨ä½œ æ³›åŒ–æµ‹è¯•æº
    file_test_full = os.path.join(OUTPUT_DIR, "test_full.jsonl")
    df_test.to_json(file_test_full, orient='records', lines=True, force_ascii=False)

    # æ‰“å°å…¨é‡ç»Ÿè®¡
    print_detailed_report(df_train, "Train Full Source (Used for RAG filtering)")
    print_detailed_report(df_test, "Test Full Source (Unseen Generalization)")

    # 3. æŠ½æ ·é€»è¾‘ (ç”Ÿæˆç”¨äºè®­ç»ƒ LightGBM æ ¡å‡†å™¨çš„æ•°æ®é›†)
    # é€»è¾‘: å– Train çš„ 1/8 -> æŒ‰ 2:3 æ­£è´Ÿæ¯”ä¾‹é‡ç»„ -> æ­£æ ·æœ¬æŒ‰ CWE åŠ æƒ

    print("\nğŸ—ï¸  Constructing Calibration Dataset (Sub-sampling Train)...")

    total_train = len(df_train)
    target_subset_size = total_train // 8

    # 2:3 æ¯”ä¾‹ => 40% Vuln, 60% Safe
    target_pos = int(target_subset_size * 0.40)
    target_neg = target_subset_size - target_pos

    pool_pos = df_train[df_train['target'] == 1]
    pool_neg = df_train[df_train['target'] == 0]

    print(f"   -> Target Total Size : {target_subset_size}")
    print(f"   -> Target Positive   : {target_pos} (40%)")
    print(f"   -> Target Negative   : {target_neg} (60%)")

    # æ­£æ ·æœ¬å¤„ç†
    if len(pool_pos) < target_pos:
        print("âš ï¸ [Warning] Not enough positives for 40%. Taking ALL available positives.")
        final_pos = pool_pos
        # ç»´æŒ 2:3 æ¯”ä¾‹è°ƒæ•´è´Ÿæ ·æœ¬ => Neg = Pos * (3/2) = Pos * 1.5
        target_neg = int(len(final_pos) * 1.5)
        print(f"   -> Adjusted Negative Target: {target_neg}")
    else:
        # æ„é€ æŠ½æ ·æƒé‡
        temp_cwe_col = pool_pos['cwe'].apply(clean_cwe_for_stats)
        counts = temp_cwe_col.value_counts()
        # å¹³æ»‘å¤„ç†ï¼šæƒé‡ = sqrt(count)
        weights_map = counts ** 0.5
        weights_map = weights_map / weights_map.sum()

        # æ˜ å°„å›æ¯ä¸€è¡Œ
        row_weights = temp_cwe_col.map(weights_map)

        final_pos = pool_pos.sample(n=target_pos, weights=row_weights, random_state=SEED)

    # è´Ÿæ ·æœ¬å¤„ç† (çº¯éšæœºï¼Œæ¨¡æ‹ŸçœŸå®å™ªéŸ³åˆ†å¸ƒ)
    final_neg = pool_neg.sample(n=target_neg, random_state=SEED)

    # åˆå¹¶ + Shuffle
    df_calibration = pd.concat([final_pos, final_neg]).sample(frac=1, random_state=SEED)

    # ä¿å­˜æ ¡å‡†æ•°æ®é›†
    file_calibration = os.path.join(OUTPUT_DIR, "confidence_train.jsonl")
    df_calibration.to_json(file_calibration, orient='records', lines=True, force_ascii=False)

    print_detailed_report(df_calibration, "Final Calibration Train Set")

    print(f"\nâœ… All artifacts generated in '{OUTPUT_DIR}':")
    print(f"   1. {file_train_full} (Use for RAG - Build logic handles filtering)")
    print(f"   2. {file_test_full} (Use for Evaluation)")
    print(f"   3. {file_calibration} (Use for Offline Static Analysis -> Calibration Training)")


if __name__ == "__main__":
    main()