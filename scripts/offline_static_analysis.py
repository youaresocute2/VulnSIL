# scripts/offline_static_analysis.py
import sys
import os
import json
import logging
import typer
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from vulnsil.database import SessionLocal, engine, get_db_session  # ä¿®å¤ï¼šå¼•å…¥ get_db_session
from vulnsil.models import StaticAnalysisCache, Vulnerability, Base
from vulnsil.core.static_analysis.engine import DualEngineAnalyzer
from vulnsil.core.static_analysis.ast_analyzer import ASTHeuristicAnalyzer  # æ–°å¢ï¼šAST ç‰¹å¾
from config import settings

# åˆå§‹åŒ–æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(name)s | %(message)s'
)
logger = logging.getLogger("OfflineEngine")

app = typer.Typer()


def analyze_chunk_worker(tasks_chunk):
    """
    Worker è¿›ç¨‹æ‰§è¡Œé™æ€åˆ†æï¼š
    - ä½¿ç”¨ DualEngineAnalyzerï¼ˆJoern + c2cpg ç­‰ï¼‰
    - ä½¿ç”¨ ASTHeuristicAnalyzerï¼ˆTree-sitterï¼‰è¡¥å……å±é™© API + å›¾å¯†åº¦ç‰¹å¾
    è¿”å›çš„åˆ—è¡¨å…ƒç´ ç»“æ„ï¼š
    {
        "task_name": str,
        "source_type": int,
        "feature_json": json.dumps({
            "has_flow": bool,
            "complexity": int,
            "apis": List[str],
            "ast_has_dangerous": bool,
            "graph_density": float,
            "api_count": int
        })
    }
    """
    try:
        analyzer = DualEngineAnalyzer()
    except Exception as e:
        # å¦‚æœåˆå§‹åŒ–å¤±è´¥ï¼Œæ•´ä¸ª chunk è·³è¿‡
        logger.error(f"[Worker] DualEngineAnalyzer init failed: {e}")
        return []

    try:
        ast_analyzer = ASTHeuristicAnalyzer()
    except Exception as e:
        logger.error(f"[Worker] ASTHeuristicAnalyzer init failed: {e}")
        ast_analyzer = None

    engine_input = []
    name_map = {}
    code_map = {}

    # æ„é€  batch è¾“å…¥
    for i, t in enumerate(tasks_chunk):
        tmp_id = 10000 + i  # æœ¬åœ°ä¸´æ—¶ idï¼Œä¸å…¥åº“
        code = t.get("code", "") or ""
        name = t.get("name")

        engine_input.append({"id": tmp_id, "code": code})
        name_map[tmp_id] = name
        code_map[tmp_id] = code

    # è°ƒç”¨ DualEngineAnalyzer æ‰¹é‡åˆ†æ
    try:
        raw_results = analyzer.analyze_batch(engine_input)
    except Exception as e:
        logger.error(f"[Worker] analyze_batch failed: {e}")
        return []

    processed = []
    for tmp_id, features in raw_results.items():
        original_name = name_map.get(tmp_id)
        if not original_name:
            continue

        # --- ä» DualEngineAnalyzer æå–ç‰¹å¾ ---
        s_type = features.get("source_type", 0)
        has_flow = bool(features.get("has_data_flow", False))
        complexity = int(features.get("complexity", 0))
        apis_from_engine = features.get("apis", []) or []

        # --- ä½¿ç”¨ ASTHeuristicAnalyzer è¡¥å……å±é™© API + å›¾å¯†åº¦ ---
        ast_has_dangerous = False
        graph_density = 0.0
        ast_apis = []

        code = code_map.get(tmp_id, "")
        if ast_analyzer is not None and code:
            try:
                ast_has_dangerous, ast_apis, graph_density = ast_analyzer.scan(code)
            except Exception:
                # AST åˆ†æå¤±è´¥æ—¶ä¸å½±å“æ•´ä½“æµç¨‹
                ast_has_dangerous = False
                graph_density = 0.0
                ast_apis = []

        # åˆå¹¶ API åˆ—è¡¨ï¼ˆå»é‡ï¼‰
        merged_apis_set = set(apis_from_engine) | set(ast_apis)
        merged_apis = sorted(list(merged_apis_set))
        api_count = len(merged_apis)

        feat_payload = {
            "has_flow": has_flow,                       # æ¥è‡ª Joern æ•°æ®æµåˆ†æ
            "complexity": complexity,                   # å¤æ‚åº¦
            "apis": merged_apis,                        # ç»¼åˆé™æ€åˆ†æ & AST æå–çš„å±é™© API
            "ast_has_dangerous": ast_has_dangerous,     # AST æ˜¯å¦æ£€æµ‹åˆ°å±é™©å‡½æ•°
            "graph_density": float(graph_density),      # Tree-sitter AST ç®€å•å›¾å¯†åº¦
            "api_count": api_count                      # å±é™© API æ•°é‡
        }

        processed.append({
            "task_name": original_name,
            "source_type": s_type,
            "feature_json": json.dumps(feat_payload)
        })

    return processed


def save_chunk_to_db(records):
    """
    å°†ä¸€æ‰¹é™æ€åˆ†æç»“æœå†™å…¥ StaticAnalysisCache è¡¨ã€‚
    """
    if not records:
        return

    sess = SessionLocal()
    try:
        for r in records:
            cache_obj = StaticAnalysisCache(
                task_name=r["task_name"],
                source_type=r["source_type"],
                feature_json=r["feature_json"]
            )
            sess.add(cache_obj)
        sess.commit()
    except Exception as e:
        sess.rollback()
        logger.error(f"DB Save Failed: {e}")
    finally:
        sess.close()


@app.command()
def analyze_split(
    split_name: str = typer.Option(..., help="Dataset prefix (e.g., 'diversevul_train')"),
    limit: int = typer.Option(-1, help="Limit number of tasks to analyze (default: all)"),
):
    """
    å¯¹æŒ‡å®š split å‰ç¼€çš„ Vulnerability è®°å½•åšç¦»çº¿é™æ€åˆ†æï¼Œå¹¶å†™å…¥ StaticAnalysisCacheã€‚

    ç¤ºä¾‹ï¼š
        python scripts/offline_static_analysis.py analyze-split \\
            --split-name diversevul_train \\
            --limit 20000
    """
    # ç¡®ä¿è¡¨ç»“æ„å­˜åœ¨
    Base.metadata.create_all(bind=engine)

    # 1. æŸ¥è¯¢å·²æœ‰ç¼“å­˜çš„ task_name åˆ—è¡¨ï¼ˆä½¿ç”¨ get_db_sessionï¼Œé¿å… session ç”Ÿå‘½å‘¨æœŸé—®é¢˜ï¼‰
    with get_db_session() as sess:
        logger.info(f"Scanning existing StaticAnalysisCache for split: {split_name}")
        cached_names = [
            r[0]
            for r in sess.query(StaticAnalysisCache.task_name)
            .filter(StaticAnalysisCache.task_name.like(f"{split_name}%"))
            .all()
        ]
        cached_set = set(cached_names)
        logger.info(f"Cached Records Found: {len(cached_set)}")

        # 2. è·å–è¯¥ split ä¸‹æ‰€æœ‰å¾…åˆ†æä»»åŠ¡
        query = (
            sess.query(Vulnerability.name, Vulnerability.code)
            .filter(Vulnerability.name.like(f"{split_name}%"))
        )

        todo_list = []
        total_scanned = 0

        logger.info("Scanning for uncached tasks...")
        # ä½¿ç”¨ yield_per é¿å…ä¸€æ¬¡æ€§åŠ è½½è¿‡å¤šè¡Œ
        for row in query.yield_per(1000):
            total_scanned += 1
            if row.name not in cached_set:
                if row.code:
                    todo_list.append({"name": row.name, "code": row.code})

                # Limit æ§åˆ¶é€»è¾‘
                if limit > 0 and len(todo_list) >= limit:
                    logger.info(f"ğŸ›‘ Reached limit of {limit} pending tasks. Stopping scan.")
                    break

    count_todo = len(todo_list)
    logger.info(f"Total Scanned Tasks : {total_scanned}")
    logger.info(f"Pending Analysis    : {count_todo}")

    if count_todo == 0:
        logger.info("âœ… No new tasks to analyze.")
        return

    # 3. é…ç½®å¤šè¿›ç¨‹æ± 
    BATCH_SIZE = settings.STATIC_ANALYSIS_BATCH_SIZE
    # ç»™ç³»ç»Ÿå’Œå…¶ä»–æœåŠ¡ç•™ä¸€ç‚¹ CPU
    MAX_WORKERS = max(1, (os.cpu_count() or 4) - 2)

    logger.info(f"Launching Pool: {MAX_WORKERS} workers, Batch Size: {BATCH_SIZE}")

    batches = [todo_list[i:i + BATCH_SIZE] for i in range(0, count_todo, BATCH_SIZE)]

    # 4. å¤šè¿›ç¨‹æ‰§è¡Œé™æ€åˆ†æ + å†™å…¥ DB
    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as pool:
        future_to_batch = {pool.submit(analyze_chunk_worker, b): b for b in batches}

        with tqdm(total=count_todo, unit="func", desc="Analyzing") as pbar:
            for future in as_completed(future_to_batch):
                batch = future_to_batch[future]
                try:
                    result_records = future.result()
                    save_chunk_to_db(result_records)
                    pbar.update(len(batch))
                except Exception as e:
                    logger.error(f"Chunk processing failed: {e}")
                    pbar.update(len(batch))

    logger.info("ğŸ‰ Offline Analysis Complete.")


if __name__ == "__main__":
    app()
