# scripts/tune_threshold.py
import sys
import os
import numpy as np
import typer
from tqdm import tqdm
from sklearn.metrics import precision_recall_curve, f1_score, precision_score, recall_score, confusion_matrix
from sklearn.utils import resample

# é€‚é…è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from vulnsil.database import get_db_session
from vulnsil.models import AnalysisResultRecord, Vulnerability
from vulnsil.utils_log import setup_logging

app = typer.Typer()
log = setup_logging("tune_threshold_bootstrap")


def find_optimal_threshold(y_true, y_scores):
    """
    Given a set of labels and scores, mathematically determine the threshold
    that yields the absolute maximum F1 score using Vectorized operations.
    """
    # Precision-Recall Curve è‡ªåŠ¨è®¡ç®—æ¯ä¸ªç‹¬ç‰¹scoreç‚¹çš„é˜ˆå€¼
    precisions, recalls, thresholds = precision_recall_curve(y_true, y_scores)

    # é¿å…åˆ†æ¯ä¸º0
    denominator = precisions + recalls
    with np.errstate(divide='ignore', invalid='ignore'):
        f1_scores = 2 * (precisions * recalls) / denominator
        f1_scores = np.nan_to_num(f1_scores)  # æ›¿æ¢ nan

    # æ‰¾åˆ°æœ€å¤§ F1 çš„ç´¢å¼•
    best_idx = np.argmax(f1_scores)

    # thresholds çš„é•¿åº¦æ¯” precisions/recalls å°‘ 1ï¼ˆæœ€åä¸€ä¸ªç‚¹é€šå¸¸å¯¹åº” th=1.0ï¼‰
    if best_idx < len(thresholds):
        best_th = thresholds[best_idx]
    else:
        best_th = 0.5  # Fallback

    return best_th, f1_scores[best_idx]


@app.command()
def tune(
        split_name: str = typer.Option(..., help="Target dataset split (e.g., 'diversevul_test')"),
        n_bootstraps: int = 100,
        random_state: int = 42
):
    """
    [Improved] Determine Robust Decision Threshold using Bootstrap Aggregation.
    Formula: th_hat = E[arg max F1(y_b, probs_b)] over 100 resamples.
    Helps avoid overfitting to sparse positive samples in imbalanced datasets.
    """
    log.info(f"ğŸ” Loading data for split: {split_name} ...")

    # 1. ä»æ•°æ®åº“è¯»å–æ•°æ®
    with get_db_session() as db:
        results = db.query(
            Vulnerability.ground_truth_label,
            AnalysisResultRecord.calibrated_confidence
        ).join(AnalysisResultRecord).filter(
            Vulnerability.name.like(f"{split_name}%"),
            Vulnerability.status == "Success"
        ).all()

    if not results:
        log.error("âŒ No records found.")
        return

    # æ•°æ®å‡†å¤‡
    y_true_full = np.array([r[0] for r in results])
    # å¡«å…… None å€¼ï¼Œç¡®ä¿ shape ä¸€è‡´
    y_scores_full = np.array([r[1] if r[1] is not None else 0.0 for r in results])

    total = len(y_true_full)
    pos_count = np.sum(y_true_full)
    pos_ratio = pos_count / total

    print("\n" + "=" * 60)
    print(f" ğŸ“Š BOOTSTRAP THRESHOLD TUNING")
    print("=" * 60)
    print(f" Dataset         : {split_name}")
    print(f" Samples         : {total}")
    print(f" Vuln Count      : {pos_count} ({pos_ratio:.2%})")
    print(f" Bootstrap Rounds: {n_bootstraps}")
    print("-" * 60)

    # 2. Bootstrap Loop
    bootstrap_thresholds = []

    # ç¡®ä¿éšæœºæ€§å¯å¤ç°
    rng = np.random.RandomState(random_state)

    log.info("ğŸ”„ Running Bootstrap Resampling...")

    with tqdm(total=n_bootstraps, unit="round") as pbar:
        for i in range(n_bootstraps):
            # Resample (æœ‰æ”¾å›é‡‡æ ·)ï¼Œå¿…é¡»ä½¿ç”¨ stratify ä»¥é˜²ç¨€ç–åå·®å¯¼è‡´æŸäº›æ‰¹æ¬¡æ— æ­£æ ·æœ¬
            # å¦‚æœæ­£æ ·æœ¬æå°‘ï¼Œresample å¯èƒ½ä¼šé‡‡åˆ°å…¨æ˜¯ 0 çš„æ ·æœ¬ï¼Œstratify èƒ½ç¼“è§£
            try:
                # æ³¨ï¼šå¦‚æœæ€»æ ·æœ¬è¿‡å°‘ï¼Œstratify å¯èƒ½ä¼šæŠ¥é”™ï¼Œè¿™é‡ŒåŠ ä¸ªç®€å•åˆ¤æ–­
                stratify_param = y_true_full if pos_count > 5 else None

                y_b, probs_b = resample(
                    y_true_full,
                    y_scores_full,
                    n_samples=len(y_true_full),
                    replace=True,
                    stratify=stratify_param,
                    random_state=rng.randint(0, 100000)
                )

                # åœ¨è¯¥ Boot å­é›†ä¸Šå¯»æ‰¾æœ€ä½³ F1 å¯¹åº”çš„é˜ˆå€¼
                th, _ = find_optimal_threshold(y_b, probs_b)
                bootstrap_thresholds.append(th)

            except Exception as e:
                # æç«¯æƒ…å†µå®¹é”™
                pass

            pbar.update(1)

    if not bootstrap_thresholds:
        log.error("Bootstrap failed to find valid thresholds.")
        return

    # 3. ç»Ÿè®¡èšåˆ
    # E[arg max F1]
    final_robust_th = np.mean(bootstrap_thresholds)
    th_std = np.std(bootstrap_thresholds)

    # è®¡ç®—ç½®ä¿¡åŒºé—´ (95% CI)
    ci_lower = np.percentile(bootstrap_thresholds, 2.5)
    ci_upper = np.percentile(bootstrap_thresholds, 97.5)

    print("\n" + "-" * 60)
    print(" ğŸ“ˆ BOOTSTRAP RESULTS")
    print("-" * 60)
    print(f" Mean Optimal Threshold (Robust): {final_robust_th:.4f}")
    print(f" Threshold Std Dev              : {th_std:.4f}")
    print(f" 95% Confidence Interval        : [{ci_lower:.4f}, {ci_upper:.4f}]")
    print("=" * 60 + "\n")

    # 4. éªŒè¯ç¯èŠ‚ï¼šå°† Robust Threshold åº”ç”¨å›å…¨é‡æ•°æ®ï¼ŒæŸ¥çœ‹é¢„æœŸè¡¨ç°
    # åº”ç”¨æœ€ç»ˆè®¡ç®—å‡ºçš„å‡å€¼é˜ˆå€¼
    y_pred_robust = (y_scores_full >= final_robust_th).astype(int)

    f1 = f1_score(y_true_full, y_pred_robust, zero_division=0)
    prec = precision_score(y_true_full, y_pred_robust, zero_division=0)
    rec = recall_score(y_true_full, y_pred_robust, zero_division=0)
    tn, fp, fn, tp = confusion_matrix(y_true_full, y_pred_robust).ravel()

    print(f"ğŸ† EXPECTED PERFORMANCE ON FULL DATASET (Using TH = {final_robust_th:.4f})")
    print(f"   F1-Score  : {f1:.4f}")
    print(f"   Precision : {prec:.4f}")
    print(f"   Recall    : {rec:.4f}")
    print(f"   Matrix    : TN={tn}, FP={fp}, FN={fn}, TP={tp}")
    print(f"\nğŸ’¡ Recommendation: update 'CALIBRATION_THRESHOLD' in config.py to {final_robust_th:.4f}")


if __name__ == "__main__":
    app()