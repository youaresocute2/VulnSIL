# scripts/manage_database.py
import sys
import os
import json
import argparse
import logging
from tqdm import tqdm

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from vulnsil.database import get_db_session, engine, Base
# å¼•å…¥æ‰€æœ‰æ¨¡å‹ä»¥ç¡®ä¿ SQLAlchemy èƒ½æ­£ç¡®è¯†åˆ«è¡¨å…³ç³»
from vulnsil.models import Vulnerability, StaticAnalysisCache, KnowledgeBase, Prediction
from config import settings, init_runtime

# åˆå§‹åŒ–æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(name)s | %(message)s'
)
log = logging.getLogger("ManageDB")


def init_db():
    """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„"""
    Base.metadata.create_all(bind=engine)
    log.info(f"Checking Schema at: {settings.DATABASE_URI}")


def perform_cleanup(db, mode):
    """
    æ ¹æ®ä¸åŒæ¨¡å¼æ‰§è¡Œæ•°æ®æ¸…ç†
    """
    # æ¨¡å¼ 0: ç‰©ç†é‡å»º (å±é™©æ“ä½œï¼Œä¹Ÿä¼šæ¸…ç©º RAG)
    if mode == 'recreate':
        log.warning("ğŸ”¥ RECREATE MODE: Deleting DB file (ALL DATA INCLUDING RAG WILL BE LOST)...")
        if "sqlite" in settings.DATABASE_URI:
            f = settings.DATABASE_URI.replace("sqlite:///", "")
            try:
                if os.path.exists(f): os.remove(f)
                if os.path.exists(f + "-wal"): os.remove(f + "-wal")
                if os.path.exists(f + "-shm"): os.remove(f + "-shm")
            except Exception as e:
                log.error(f"Remove failed: {e}")
        init_db()
        return

    # äº‹åŠ¡æ“ä½œ
    try:
        # æ¨¡å¼ 1: ä»…æ¸…ç©ºåˆ†æç»“æœ (--clear_results)
        # è¡Œä¸º: åˆ é™¤æ¨ç†ç»“æœï¼Œé‡ç½®ä»»åŠ¡çŠ¶æ€ã€‚
        # ä¿ç•™: æ¼æ´ä»»åŠ¡(Vulnerability), é™æ€åˆ†æ(StaticAnalysisCache), RAG(KnowledgeBase)
        if mode == 'results_only':
            log.info("Action: Clearing Analysis Results...")
            deleted_res = db.query(Prediction).delete()

            log.info("Action: Resetting Vulnerability status to 'Pending'...")
            updated_vulns = db.query(Vulnerability).update(
                {Vulnerability.status: "Pending"},
                synchronize_session=False
            )
            log.info(f"Deleted {deleted_res} results. Reset {updated_vulns} tasks.")
            db.commit()
            return

        # æ¨¡å¼ 2: æ¸…ç©ºç»“æœ + é™æ€ç¼“å­˜ (--clear_vulns)
        # è¡Œä¸º: åˆ é™¤ç»“æœ + é™æ€ç¼“å­˜ï¼Œé‡ç½®ä»»åŠ¡çŠ¶æ€ã€‚
        # ä¿ç•™: æ¼æ´ä»»åŠ¡(Vulnerability), RAG(KnowledgeBase)
        if mode == 'results_and_static':
            log.info("Action: Clearing Analysis Results...")
            deleted_res = db.query(Prediction).delete()

            log.info("Action: Clearing Static Cache...")
            deleted_cache = db.query(StaticAnalysisCache).delete()

            log.info("Action: Resetting Vulnerability status to 'Pending'...")
            updated_vulns = db.query(Vulnerability).update(
                {Vulnerability.status: "Pending"},
                synchronize_session=False
            )
            log.info(f"Deleted {deleted_res} results, {deleted_cache} cache. Reset {updated_vulns} tasks.")
            db.commit()
            return

        # æ¨¡å¼ 3: æ¸…ç©ºç»“æœ + é™æ€ç¼“å­˜ + ä»»åŠ¡ (--clear_all)
        # è¡Œä¸º: åˆ é™¤ç»“æœ + é™æ€ç¼“å­˜ + ä»»åŠ¡ã€‚
        # ä¿ç•™: RAG(KnowledgeBase)
        if mode == 'all_tasks':
            log.info("Action: Clearing Analysis Results...")
            deleted_res = db.query(Prediction).delete()

            log.info("Action: Clearing Static Cache...")
            deleted_cache = db.query(StaticAnalysisCache).delete()

            log.info("Action: Clearing Vulnerabilities...")
            deleted_vulns = db.query(Vulnerability).delete()

            log.info(f"Deleted {deleted_res} results, {deleted_cache} cache, {deleted_vulns} tasks.")
            db.commit()
            return

        # æ¨¡å¼ 4: æ¸…ç©ºæ‰€æœ‰ï¼ŒåŒ…æ‹¬ RAG (--clear_all_including_rag)
        if mode == 'all_including_rag':
            log.info("Action: Clearing Analysis Results...")
            deleted_res = db.query(Prediction).delete()

            log.info("Action: Clearing Static Cache...")
            deleted_cache = db.query(StaticAnalysisCache).delete()

            log.info("Action: Clearing Vulnerabilities...")
            deleted_vulns = db.query(Vulnerability).delete()

            log.info("Action: Clearing Knowledge Base...")
            deleted_kb = db.query(KnowledgeBase).delete()

            log.info(f"Deleted {deleted_res} results, {deleted_cache} cache, {deleted_vulns} tasks, {deleted_kb} KB entries.")
            db.commit()
            return

    except Exception as e:
        db.rollback()
        log.error(f"Cleanup Failed: {e}")


def import_jsonl(db, file_path: str, split_name: str):
    """
    ä» JSONL æ–‡ä»¶å¯¼å…¥ Vulnerability æ•°æ®
    """
    records = parse_json_file(file_path)
    buffer = []
    batch_size = 500

    for rec in tqdm(records, desc="Importing"):
        commit_id = rec.get('commit_id', None)
        name = f"{split_name}_{commit_id}_{rec.get('idx', 'unk')}"

        obj = Vulnerability(
            name=name,
            commit_id=commit_id,
            code=rec.get('func', ''),
            ground_truth_label=rec.get('target', 0),
            cwe_id=rec.get('cwe', 'N/A')
        )
        buffer.append(obj)

        if len(buffer) >= batch_size:
            db.bulk_save_objects(buffer)
            db.commit()
            buffer = []

    if buffer:
        db.bulk_save_objects(buffer)
        db.commit()

    log.info(f"Import {split_name} successfully completed.")


def parse_json_file(filepath: str):
    """è§£æ JSONL æ–‡ä»¶"""
    records = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                try:
                    records.append(json.loads(line))
                except json.JSONDecodeError:
                    pass
    return records


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="VulnSIL Database Management Tool")

    # æ¸…ç†å‚æ•°ç»„ (äº’æ–¥)
    cleanup_group = parser.add_mutually_exclusive_group()
    cleanup_group.add_argument("--clear_results", action="store_true",
                               help="[Mode 1] Clear LLM analysis results only. Keep tasks, static cache, and RAG.")
    cleanup_group.add_argument("--clear_vulns", action="store_true",
                               help="[Mode 2] Clear results AND static analysis cache. Keep tasks and RAG.")
    cleanup_group.add_argument("--clear_all", action="store_true",
                               help="[Mode 3] Clear results, static cache, AND tasks. Keep RAG only.")
    cleanup_group.add_argument("--recreate", action="store_true",
                               help="[Mode 0] DELETE DB FILE and re-init. WARNING: Clears EVERYTHING including RAG.")

    # å¯¼å…¥å‚æ•°
    parser.add_argument("--import_file", type=str, help="Path to JSONL file to import")
    parser.add_argument("--split_name", type=str,
                        help="Dataset split prefix (Required for import), e.g. diversevul_test")

    args = parser.parse_args()

    init_runtime()

    # 1. å¤„ç† Recreate
    if args.recreate:
        perform_cleanup(None, 'recreate')
    else:
        # ç¡®ä¿è¡¨å­˜åœ¨
        init_db()

        with get_db_session() as db:
            # 2. å¤„ç†ä¸‰ç§æ¸…ç†æ¨¡å¼
            if args.clear_results:
                perform_cleanup(db, 'results_only')
            elif args.clear_vulns:
                perform_cleanup(db, 'results_and_static')
            elif args.clear_all:
                perform_cleanup(db, 'all_tasks')

            # 3. å¤„ç†å¯¼å…¥
            if args.import_file:
                if not args.split_name:
                    log.error("--split_name is required when importing!")
                    exit(1)
                import_jsonl(db, args.import_file, args.split_name)