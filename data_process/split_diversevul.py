# data_process/split_diversevul.py
import json
import os
import hashlib
import pandas as pd
import numpy as np
import sys
from collections import Counter
from config import settings  # æ–°å¢ï¼šç»Ÿä¸€å‚æ•°

# === è·¯å¾„é…ç½® ===
# è¯·ç¡®è®¤è¿™æ˜¯æ‚¨æœ¬åœ°çš„åŸå§‹æ•°æ®é›†è·¯å¾„
INPUT_FILE = "data/diversevul.json"
# è¾“å‡ºç›®å½•
OUTPUT_DIR = "data/dataset_final"

# éšæœºç§å­ï¼Œä¿è¯æ¯æ¬¡è¿è¡Œç»“æœä¸€è‡´
SEED = 42

os.makedirs(OUTPUT_DIR, exist_ok=True)


def get_hash_split_label(commit_id, ratio=0.8):
    """
    Commit-level ç‰©ç†éš”ç¦» (Hashå–æ¨¡)
    """
    if not commit_id: return "train"
    # ä½¿ç”¨ sha256 é¿å… MD5 ç¢°æ’é£é™©ï¼Œè™½ç„¶åœ¨å–æ¨¡100çš„æƒ…å†µä¸‹åŒºåˆ«ä¸å¤§
    h = int(hashlib.sha256(str(commit_id).encode('utf-8')).hexdigest(), 16)
    return "train" if (h % 100) < (ratio * 100) else "test"


def clean_cwe_for_stats(raw_val):
    """
    ä»…ç”¨äºPandasç»Ÿè®¡å’Œåˆ†å±‚æŠ½æ ·è®¡ç®—ï¼Œä¸ä¿®æ”¹åŸå§‹æ•°æ®ç»“æ„
    """
    if isinstance(raw_val, list) and len(raw_val) > 0:
        val = str(raw_val[0])
    elif isinstance(raw_val, str):
        val = raw_val
    else:
        val = "Other"

    val = val.strip().upper()
    if val in ["NON", "N/A", "NONE", "NULL", "", "[]"]: return "Other"
    return val


def print_detailed_report(df, name):
    """
    è¯¦ç»†çš„æ•°æ®ç»Ÿè®¡é¢æ¿
    """
    total = len(df)
    if total == 0:
        print(f"\nâŒ [{name}] is Empty!")
        return

    n_vuln = len(df[df['target'] == 1])
    n_safe = len(df[df['target'] == 0])

    vuln_rate = (n_vuln / total) * 100
    safe_rate = (n_safe / total) * 100

    ratio_str = f"1 : {n_safe / n_vuln:.2f}" if n_vuln > 0 else "N/A"

    print("\n" + "=" * 60)
    print(f" ğŸ“Š DATASET REPORT: {name}")
    print("=" * 60)
    print(f" ğŸ“¦ Total Rows : {total}")
    print(f" ğŸ”´ Vuln (1)   : {n_vuln:<8} ({vuln_rate:.2f}%)")
    print(f" ğŸŸ¢ Safe (0)   : {n_safe:<8} ({safe_rate:.2f}%)")
    print(f" âš–ï¸ Ratio      : {ratio_str}")
    print("=" * 60 + "\n")


def main():
    """
    ä¸»å‡½æ•°ï¼šåˆ‡åˆ†æ•°æ®é›† [æ”¹è¿›] åŠ æ—¶é—´åˆ‡åˆ†ï¼ˆå‡è®¾æœ‰'date'ï¼‰
    """
    with open(INPUT_FILE, 'r') as f:
        raw_data = [json.loads(line) for line in f if line.strip()]

    # [æ”¹è¿›] æ—¶é—´åˆ‡åˆ†ï¼ˆå‡è®¾æœ‰'date'ï¼Œå¦åˆ™ç”¨commit_idä»£ç†æ—¶é—´é¡ºåºï¼‰
    # æ’åºdate (å‡è®¾æ ¼å¼'YYYY-MM-DD')
    raw_data.sort(key=lambda x: x.get('date', '0000-00-00'))

    time_split_idx = int(len(raw_data) * settings.TIME_SPLIT_RATIO)
    time_train = raw_data[:time_split_idx]
    time_test = raw_data[time_split_idx:]

    # å“ˆå¸Œåˆ‡åˆ†ç»“åˆ
    train_full = [d for d in time_train if get_hash_split_label(d.get('commit_id', '')) == "train"]
    test_full = [d for d in time_test if get_hash_split_label(d.get('commit_id', '')) == "test"]

    # åŸç‰ˆå¹³è¡¡é‡‡æ · (calibration)
    df_train = pd.DataFrame(train_full)
    pool_pos = df_train[df_train['target'] == 1]
    pool_neg = df_train[df_train['target'] == 0]

    target_pos = int(0.4 * len(df_train))
    target_neg = int(0.6 * len(df_train))

    # æ­£æ ·æœ¬å¤„ç†
    if len(pool_pos) < target_pos:
        print("âš ï¸ [Warning] Not enough positives for 40%. Taking ALL available positives.")
        final_pos = pool_pos
        target_neg = int(len(final_pos) * 1.5)
        print(f"   -> Adjusted Negative Target: {target_neg}")
    else:
        # æ„é€ æŠ½æ ·æƒé‡
        temp_cwe_col = pool_pos['cwe'].apply(clean_cwe_for_stats)
        counts = temp_cwe_col.value_counts()
        # å¹³æ»‘å¤„ç†ï¼šæƒé‡ = sqrt(count)
        weights_map = counts ** 0.5
        weights_map = weights_map / weights_map.sum()

        # æ˜ å°„å›æ¯ä¸€è¡Œ
        row_weights = temp_cwe_col.map(weights_map)

        final_pos = pool_pos.sample(n=target_pos, weights=row_weights, random_state=SEED)

    # è´Ÿæ ·æœ¬å¤„ç† (çº¯éšæœºï¼Œæ¨¡æ‹ŸçœŸå®å™ªéŸ³åˆ†å¸ƒ)
    final_neg = pool_neg.sample(n=target_neg, random_state=SEED)

    # åˆå¹¶ + Shuffle
    df_calibration = pd.concat([final_pos, final_neg]).sample(frac=1, random_state=SEED)

    # ä¿å­˜
    file_train_full = os.path.join(OUTPUT_DIR, "diversevul_train.jsonl")
    df_train.to_json(file_train_full, orient='records', lines=True, force_ascii=False)

    file_test_full = os.path.join(OUTPUT_DIR, "diversevul_test.jsonl")
    pd.DataFrame(test_full).to_json(file_test_full, orient='records', lines=True, force_ascii=False)

    file_calibration = os.path.join(OUTPUT_DIR, "confidence_train.jsonl")
    df_calibration.to_json(file_calibration, orient='records', lines=True, force_ascii=False)

    print_detailed_report(df_train, "Final Train Set")
    print_detailed_report(pd.DataFrame(test_full), "Final Test Set")
    print_detailed_report(df_calibration, "Final Calibration Train Set")

    print(f"\nâœ… All artifacts generated in '{OUTPUT_DIR}':")
    print(f"   1. {file_train_full} (Use for RAG - Build logic handles filtering)")
    print(f"   2. {file_test_full} (Use for Evaluation)")
    print(f"   3. {file_calibration} (Use for Offline Static Analysis -> Calibration Training)")


if __name__ == "__main__":
    main()